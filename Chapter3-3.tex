%TODO Review this chapter entirely, only read once.
%TODO link public github?

\chapter{Simulated Annealing Algorithm}
\label{ch:SA}

To fit parameters to the data a fitting algorithm had to be chosen. For almost all of the fitting we used a simulated annealing algorithm. In this chapter we will explain the inner workings of this algorithm and we will explain why we specifically used this algorithm.



\section{Basic principles of simulated annealing}

\cite{SA} %TODO ...

Simulated annealing is a form of an optimization algorithm. It draws inspiration from statistical mechanics. In statistical mechanics systems with many particles and therefore many degrees of freedom reach a thermal equilibrium. This behaviour is similar to a function with many degrees of freedom for which one tries to find the global minimum.

In statistical mechanics a system can be in a finite number of states. Each of these states has a specific energy associated with it. In thermal equilibrium the probability to be in any of these states follows the boltzmann distribution, as we have discussed in section \ref{seq:EquilTheory}. Naturally then, if a system is cooled down to a low enough temperature the only occupied state will be the state with the lowest energy. In the optimization analogy this lowest energy state would be the minimum of the function we are trying to minimize. There is one catch to this idea; if a system is cooled down fast enough it will reach a local equilibrium but not a global equilibrium. As an example take a liquid which is cooled down so it forms a solid. If the cooling is quick, the solid will form but it will have many defects. Essentially the molecules are simply locked into place. This configuration does have less energy than the liquid state, however it is not the ground state of the system. If instead the cooling process is very gradual the liquid has the time and oppertunity to form a perfect crystal. This crystal is the actual ground state of the system. This gradual cooling is the 'annealing' part of the simulated annealing algorithm. It refers to the very gradual annealing of a crystal from a melt. By 'cooling down' the cost function gradually we are able to obtain better solutions than just the local minima.

That there is an analogy between minimizing the cost function and cooling down an ensemble of particles to its ground state is clear. However, we conveniently left out the temperature analogy up to this point. While clearly there is a temperature when cooling down a physical system, the equivalent of the temperature or the energy of the system is not as clear in the case of the cost function. In fact, simulated annealing algorithms explicitly introduce this temperature. When simulated annealing finds a better solution to the cost function, it accepts that solution until a better one is found. If only these better solutions were accepted that is akin to flash-freezing our system, it will simply move towards the nearest local minimum. Instead, if the algorithm discovers a worse solution it still has some chance to accept this solution. The probability to accept a solution is given by the following formula:

\begin{equation}
\label{eq:SAProbCost}
P_{accept} = \exp \left(-\frac{C_1-C_0}{k_BT}\right),
\end{equation}

where $T$ is the temperature, $k_B$ is the boltzmann constant, $C_0$ is the cost of the old solution and $C_1$ the cost of the new solution. The consequence of choosing this particular probability function is that the system will evolve into a boltzmann distribution over time. It is clear that if we lower the temperature in this case, it becomes harder for worse solutions than the current one to be accepted.


\section{Simulated annealing protocol}
\label{sec:SAProtocol}

The simulated annealing protocol is simple:

\begin{enumerate}
\item Define some cost function.
\item Initialize the algorithm by giving it a starting guess for the parameters and an initial temperature.
\item Vary the parameters slightly and calculate the cost.
\item Accept or reject the new parameter set based on equation \ref{eq:SAProbCost}.
\item Repeat step 3 and step 4 a number of times.
\item If the stop condition is not reached; lower the temperature and return to step 3. If the stop condition is reached continue to step 7.
\item Return the final parameter set; this should be the minimum of the cost function.
\end{enumerate}

Although this protocol is simple, there are several points which the designer of the algorithm has to take into consideration. First of all the cost function should be defined. This function is the function that is ultimately minimized (or maximized) and should be representative of the problem which is solved. Secondly the user has to initialize the algorithm with an initial guess and an initial temperature. In theory this initial guess is not important, but the initial temperature is. If the initial temperature is too low, the cost function could already be in a local minimum from the beginning and therefore never reach the global minimum of the cost function. The intial temperature should therefore be high enough that the algorithm has the oppertunity to explore all possible parameters. Thirdly the parameters should be varied slightly when choosing a new parameter set. The issue is that 'slightly' varying the parameters is relative and could also change during the running of the simulated annealing algorithm. Fourthly, the parameters should be changed a a certain number of times before lowering the temperature to simulate the waiting and settling down of the system before continuing to cool it. This effectively makes the algorithm behave quasi-equilibriated. The user should determine how long the algorithm should wait before lowering the temperature again. Finally the user should also determine the stop condition. A simple stop condition could be to stop the algorithm when it reaches a low temperature, however what constitutes a low temperature can change depending on the cost function, the data, etc.

In summary, even though the protocol is simple in essence, there are several things the user needs to consider before using a simulated annealing algorithm.


\section{Version used in this thesis}

To overcome the issues laid out in section \ref{sec:SAProtocol} the version of simulated annealing that is used in this thesis uses additional metrics and safeguards to ensure the solution found by simulated annealing is the optimal solution.

%TODO
-- Sidenote: the algorithm is not perfect, fitting is hard. --


\subsection{Cost function}

The cost function is at the heart of the simulated annealing algorithm. Since we are fitting to experimental data the cost function has to be representative of how well the model matches the input data, given a certain parameter set for the model. The metric we use for this is $\chi^2$:

\begin{equation}
\chi (x)^2 = \left( \frac{model(x)-data(x)}{error(x)} \right)^2.
\end{equation}

Here $model$ refers to the prediction of the model at point $x$, $data$ refers to the value of the data at $x$ and $error$ to the error of that specific datapoint. 
